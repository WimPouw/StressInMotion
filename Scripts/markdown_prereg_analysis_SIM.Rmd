---
title: "PreRegAnalysisSIM"
author: "Wim Pouw"
date: "5/28/2021"
output: html_document
---

```{r setup, echo=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(plotly)
library(RColorBrewer)
library(wesanderson)
library(ggbeeswarm)
library(papaja)
library(nlme)
library(plyr)
library(lsmeans)
library(tidyr)

curfolder <- getwd()
D <- read.csv(paste0(dirname(curfolder), "/ProcessedTimingData/DD.csv"))
D <- na.omit(D)

D$accent  <- ifelse(D$accent=="yes", "accent present", "no accent")
D$accent  <- factor(D$accent, levels=c("no accent", "accent present"))
D$correct <- factor(D$correct, levels=c("L2 correct", "L2 incorrect & L1 match", "L2 incorrect & L1 mismatch"))
D$stress <-  factor(D$stress, levels=c("same", "difference"))
D$condition <-  factor(D$condition, levels=c("nogesture", "gesture"))
```

## Descriptive checks

Here we provide a descriptive overview of the syllable identifications relative to target (table 1). In the current data the number of syllables identified by EasyAlign perfectly matched the targeted number of syllables, i.e., in 100% of the trials there were 0 differences in the number of syllable detected versus target. Note however that we will manually check the EasyAllign syllable detections for the final dataset.


Table 1. Percentage of syllable detection mismatches
```{r table01, echo= FALSE, warning = FALSE, message = FALSE}
tab01 <- data.frame(prop.table(table(D$Nsyllables-D$Nsyllables_correct))*100)
colnames(tab01) <- c("syllable differences", "percentage")  
  
apa_table(
  tab01
  , align = c("l", rep("r", 3))
  , caption = "A summary table of percentage of differences between syllables"
)
```

Table 2 provides the percentages of different type of L2 stress placement matches and mismatches.

Table 2. Percentage of correct L2 placements
```{r, echo = FALSE}
tab02 <- data.frame(prop.table(table(D$correct, D$stress))*100)
colnames(tab02) <- c("stress mis/match type", "stress difference", "percentage")  
  
  
apa_table(
  tab02
  , align = c("l", rep("r", 3))
  , caption = "A summary table of percentage of stress match/mismatch types"
)
```

## Main Confirmatory Analysis

## Gesture vs. no gesture

For the first analysis we simply assess whether the absolute difference in stress timing relative to the target stress time is different for the gesture or the no gesture condition.

```{r, echo = FALSE}
#stress correct
a <- ggplot(D, aes(x= stressed_mistimingL2L1, color = condition)) + geom_density(size= 2, alpha = 0.75)+theme_bw()+ylim(1e-12, NA)
a <- a + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))
ggplotly(a)
```


```{r}
D$accuracy <- abs(D$stressed_mistimingL2L1) #absolute deviation from stress from L2

#basemodel predicting the overall mean accuracy
model0 <- lme(accuracy~1, data = D, random = list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)

#alternative model with gesture versus no gesture as predictor
model1 <- lme(accuracy~condition, data = D, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
anova(model0, model1) #test difference basemodel versus model 1
sum1 <- summary(model1) 
```

<details><summary>Click here for model 1 summary</summary>
    ```{r, eval=TRUE}
    sum1
    ```
</details>
  
  
  

It is of course possible that there is a contextual effect of gesture, depending on whether there is a stress difference or a presence of an accent (see Figure 2)

Figure 2.
```{r, echo = FALSE}
#stress correct
a <- ggplot(D, aes(x= stressed_mistimingL2L1, color = condition)) + geom_density(size= 2, alpha = 0.75)+theme_bw()+ylim(1e-12, NA)+facet_grid(.~stress)
a <- a + scale_color_manual(values=wes_palette(n=2, name = "Royal1")) + ggtitle("effect gesture vs. no gesture ~ stress difference")
ggplotly(a)

#stress correct
a <- ggplot(D, aes(x= stressed_mistimingL2L1, color = condition)) + geom_density(size= 2, alpha = 0.75)+theme_bw()+ylim(1e-12, NA)+facet_grid(.~accent)
a <- a + scale_color_manual(values=wes_palette(n=2, name = "Royal1")) + ggtitle("effect gesture vs. no gesture ~ accent")
ggplotly(a)
```

We will further assess this in a complex model we expand our analysis with the relevant stimuli conditions, as well as their interactions with the gesture condition. If the interactions are statistically reliable we will perform a post-hoc comparisons with R-package "lsmeans" with a bonferroni correction.


```{r, eval = TRUE}
#alternative model with gesture versus no gesture as predictor
model2 <- lme(accuracy~condition+stress+accent, data = D, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
model3 <- lme(accuracy~condition+stress+accent+
                condition*stress+
                condition*accent, 
              data = D, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)


anova(model1, model2, model3) #test difference basemodel versus model 1

#summary model 3 post hoc
sum3 <- summary(model3)
posthoc3a <- lsmeans(model3, list(pairwise ~ condition|stress),  adjust="bonferroni")
posthoc3b <- lsmeans(model3, list(pairwise ~ condition|accent),  adjust="bonferroni")
```

<details><summary>Click here for model 3 summary</summary>
    ```{r, eval=TRUE}
    sum3
    ```
</details>
<details><summary>Click here for posth-hoc3a output</summary>
    ```{r, eval=TRUE}
    posthoc3a
    ```
</details>
<details><summary>Click here for posth-hoc3b output</summary>
    ```{r, eval=TRUE}
    posthoc3a
    ```
</details>

## Prosodic modulation of gesture

We perform a mixed linear regression with normalized acoustic markers as DV, and acoustic marker (peak F0, peak envelope, and duration) x condition as independent variable.


Figure x. 
```{r, echo = FALSE, message= FALSE, warning = FALSE, fig.width = 7}
a <- ggplot(D, aes(x = condition, y = D$peakF0z, color = condition)) + geom_quasirandom(alpha=0.2) + geom_boxplot(alpha = 0, color = "black") + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))+ylab("peak F0 (normalized)")+theme_bw()+theme(legend.position = "none")+ theme(axis.text.x=element_text(angle = -70, hjust = 0))
b <- ggplot(D, aes(x = condition, y = D$peakAMPz, color = condition)) + geom_quasirandom(alpha=0.2) + geom_boxplot(alpha = 0, color = "black") + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))+ylab("peak envelope (normalized)")+theme_bw()+theme(legend.position = "none")+ theme(axis.text.x=element_text(angle = -70, hjust = 0))
c <- ggplot(D, aes(x = condition, y = D$sDURz, color = condition)) + geom_quasirandom(alpha=0.2) + geom_boxplot(alpha = 0, color = "black") + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))+ylab("vocal duration (normalized)")+theme_bw()+theme(legend.position = "none")+ theme(axis.text.x=element_text(angle = -70, hjust = 0))
d <- ggplot(D, aes(x = condition, y = stressSCORE, color = condition)) + geom_quasirandom(alpha=0.2) + geom_boxplot(alpha = 0, color = "black") + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))+ylab("stress score (normalized)")+theme_bw()+theme(legend.position = "none")+ theme(axis.text.x=element_text(angle = -70, hjust = 0))

subplot(ggplotly(a), ggplotly(b), ggplotly(c), ggplotly(d), titleY = TRUE,margin = 0.07)

```

```{r, eval = TRUE}
Dlong <- gather(D, "marker", "acoust_out", 13:15)

#alternative model with gesture versus no gesture as predictor
model0 <- lme(acoust_out~1, data = Dlong, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
model1 <- lme(acoust_out~marker*condition, data = Dlong, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
anova(model0, model1) #test difference basemodel versus model 1

#summary model 3 post hoc
sum1 <- summary(model1)
posthocsum1 <- lsmeans(model1, list(pairwise ~ condition|marker),  adjust="bonferroni")

```
<details><summary>Click here for model 1 summary</summary>
    ```{r, eval=TRUE}
    sum1
    ```
</details>
<details><summary>Click here for posthoc model 1</summary>
    ```{r, eval=TRUE}
    posthoc3a
    ```
</details>

## Gesture-speech asynchrony as a function of trial conditions

In the previous analyses we know whether speech prosody performance increases or decreases as a function of gesture, stress difference, and accentedness. A further question is whether gesture-speech synchrony is affected by stress difference and accentedness.

Figure 3.
```{r, echo = FALSE}
subD <- subset(D, condition == "gesture")
a <- ggplot(subD, aes(x= asynchrony, color = stress)) + geom_density(size= 2)+theme_bw()+ylim(1e-12, NA)+facet_grid(.~accent)
a <- a + scale_color_manual(values=wes_palette(n=2, name = "BottleRocket2")) + ggtitle("asynchrony as a function of stress difference and accent")
ggplotly(a)
```

It seems that synchrony between gesture and speech is best in the accented and stress match condition.
    ```{r, echo=FALSE, eval=TRUE}
subD$abs_asynchrony <- abs(subD$asynchrony)
#alternative model with gesture versus no gesture as predictor
model0 <- lme(abs_asynchrony~1, data = subD, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
model1 <- lme(abs_asynchrony~stress+accent, data = subD, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
model2 <- lme(abs_asynchrony~stress*accent, data = subD, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)

anova(model0, model1, model2) #test difference basemodel versus model 1
sum2 <- summary(model2)
posthoc2 <- lsmeans(model2, list(pairwise ~ stress|accent),  adjust="bonferroni")
```

```{r, echo = FALSE, eval=FALSE}
sum2
posthoc2
```


## Gesture-speech asynchrony and the directionality of error

Now we should know whether gesture-speech synchrony can be affected by trial condition that may complicate correct stress placement. If indeed gesture-speech synchrony is affected, we can wonder about how gesture and speech diverge when they are more asynchronous. Firstly assess whether gestures

```{r, echo = FALSE}
#for correct placement
Dsub <- subset(D, condition != "gesture")
a <- ggplot(Dsub, aes(x = stress, y= asynchrony_L2L1))+geom_hline(yintercept = 0, color = "red", size = 0.5)+geom_violin(fill=NA) + geom_quasirandom(color = "black", size = 0.7, alpha = 0.5)+geom_boxplot(alpha = 0)+theme_bw() + coord_flip()
a <- a +facet_grid(correct~.)
ggplotly(a)
```


```{r}
#basemodel predicting the overall mean accuracy
model0 <- lme(asynchrony_L2L1~1, data = subD, random = list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)

#alternative model with gesture versus no gesture as predictor
model1 <- lme(asynchrony_L2L1~stress*correct, data = subD, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
anova(model0, model1) #test difference basemodel versus model 1
summary(model1) 
```


## Power analysis

https://link.springer.com/article/10.3758/s13428-021-01546-0
