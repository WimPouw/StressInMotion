---
title: "Stress in Motion: Detailed Analysis Overview"
author: "Wim Pouw"
date: "5/28/2021"
output: html_document
---

```{r setup, echo=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)      #plotting
library(plotly)       #plotting
library(RColorBrewer) #plotting
library(wesanderson)  #plotting
library(ggbeeswarm)   #plotting
library(papaja)       #markdownfeatures
library(nlme)         #linear mixed regression
library(plyr)         #data wrangling
library(lsmeans)      #post-hoc linear mixed regression
library(tidyr)        #data wrangling
library(EMAtools)     #linear mixed regression (effect size)
library(lme4)         #linear mixed regression
library(simr)       #power analysis
library(mixedpower) #power analysis
library(plyr)       #data wrangling
library(gridExtra)  #plotting

#load in the timing data
curfolder <- getwd()
D <- read.csv(paste0(dirname(curfolder), "/ProcessedTimingData/DD.csv"))
D <- na.omit(D)

#rename/code some variables
D$accent  <- ifelse(D$accent=="yes", "stress mark present", "stress mark absent")
D$accent  <- factor(D$accent, levels=c("stress mark absent", "stress mark present"))
D$correct <- factor(D$correct, levels=c("L2 correct", "L2 incorrect & L1 match", "L2 incorrect & L1 mismatch"))
D$stress <-  factor(D$stress, levels=c("same", "difference"))
D$condition <-  factor(D$condition, levels=c("nogesture", "gesture"))

#add an accuracy variable
D$accuracy <- ifelse(D$correct== "L2 correct", 1, 0)
```

## Goal of this Rmarkdown document

This Rmarkdown notebook contains a more detailed documentation of how the analysis will be performed, provided with R code when informative. It not only contains the statistical model code, but also the results they produce when applied to the pilot data. At the end of the section we report on a power analysis, providing some insights on how many participants we need to test.

## Main Research Questions

Each of the analysis are tailored to provide statistically informed conclusions about the following research questions:

* 1. How does gesturing influence the acoustic production of stress by L2 learners?

* * 1A. Does gesturing makes learners more accurate in L2 stress placement?

* * 1B. Does gesture gesturing boost the acoustic markers of stress?

* 2. How is gesture-prosody coupling in L2 influenced by competition from the speaker’s L1?

* * 2A. Is gesture-prosody sycnhrony affected by stress-match/mismatch?

* * 2B. Do correct L2 productions still demonstrate evidence of L1 temporal attraction 

Note that we will only provide an analysis for 2B, but will also perform another analysis in the eventual study.

* * 2C. Do ‘incorrect L1-like productions’ still demonstrate evidence of L2 temporal attraction in the form of a temporal bias in gestural timing in the direction of the L2 syllable.

## Initial descriptive checks of the data

Here we provide a descriptive overview of the syllable identifications relative to target (table 1). In the current pilot data the number of syllables identified by EasyAlign perfectly matched the targeted number of syllables, i.e., in 100% of the trials there were 0 differences in the number of syllable detected versus target.

```{r table01, echo= FALSE, warning = FALSE, message = FALSE}
tab01 <- data.frame(prop.table(table(D$Nsyllables-D$Nsyllables_correct))*100)
colnames(tab01) <- c("syllable differences", "percentage")  
  
apa_table(
  tab01
  , align = c("l", rep("r", 3))
  , caption = "Table 1. A summary table of percentage of differences between syllables"
)
```

Table 2 provides the percentages of different type of L2 stress placement matches and mismatches.

```{r table02, echo = FALSE}
tab02 <- data.frame(prop.table(table(D$correct, D$stress))*100)
colnames(tab02) <- c("stress mis/match type", "stress difference", "percentage")  
  
  
apa_table(
  tab02
  , align = c("l", rep("r", 3))
  , caption = "Table 2. A summary table of percentage of stress match/mismatch types"
)
```

## Main Confirmatory Analysis

For all analysis we will use mixed linear regressions with maximum likelihood estimation using R-package nlme. Our models will always have participant and trial ID as random variables. We will always try to fit random slopes, next to random intercepts. With the current pilot data however, adding random slopes resulted in non-converging models. Thus for all models reported we have participant and trial ID as random intercepts. We further report a Cohen's *D* for our model predictors using R-package EMAtools. For interaction effects we will follow up with a post-hoc contrast analysis using R-package lsmeans, and we apply a Bonferroni correction for such multiple comparison tests.

## Research question 1A:  Effect of gesture on stress placement 
  For the first analysis we simply assess whether the absolute difference in directional stress timing is different for the gesture versus no gesture condition, while also accounting for effects on timing due to stress L1/L2 difference and stress mark presence. If gesture improves stress timing, lower absolutized stress timings are to be expected (i.e., lower deviances from perfect synchrony).

Figure 3 upper panel. Effect of gesture versus no gesture on stress timing
```{r, echo = FALSE}

cdat <- ddply(D, c("condition", "ppn"), summarise, acc.percentage=sum(accuracy)/length(accuracy)) 

#stress correct
a <- ggplot(cdat, aes(x= condition, color = condition, y = acc.percentage))+geom_boxplot(alpha=0.1, size= 0.1) + geom_point(size = 4)+theme_bw()+geom_line(aes(group=ppn), color = "black")
a <- a + scale_color_manual(values=wes_palette(n=2, name = "Royal1")) 
a <- a + ylab("percentage of trials correct")


#stress correct x stress difference and stress mark present
cdat2 <- ddply(D, c("condition", "ppn", "stress", "accent"), summarise, acc.percentage=sum(accuracy)/length(accuracy)) 

b <- ggplot(cdat2, aes(x= condition, color = condition, y = acc.percentage))+geom_boxplot(alpha=0.1, size= 0.1) + geom_point(size = 4)+theme_bw()+geom_line(aes(group=ppn), color = "black")+facet_grid(stress~accent)
b <- b + scale_color_manual(values=wes_palette(n=2, name = "Royal1")) + ylab("percentage of trials correct")

grid.arrange(a, b, nrow=2)

```
  
  We first construct a base model predicting the overall mean, with participant and trial ID random variables, and absolute stress timing as the dependent variable. This model is then compared to a model with stress difference + accentedness + gesture condition as main effects.  

Code chunk 1. Model research question 1
```{r, warning = FALSE}
library(lme4)

D$ppn <- as.factor(D$ppn)
#basemodel predicting the overall mean stress timing
model0 <- glmer(accuracy ~  1 + (1 | ppn)+(1|target), data = D, family = binomial(link="logit"))

#alternative model with, stress, accentedness, and gesture versus no gesture as predictors
model1 <- glmer(accuracy ~  condition  + (1 | ppn)+(1|target),data = D, family = binomial(link="logit"))

#alternative model with, stress, accentedness, and gesture versus no gesture as predictors

model2 <- glmer(accuracy ~  condition*accent*stress + (1|ppn) + (1|target), data = D, family = binomial(link="logit"))

#compare models
anovcomp01 <- anova(model0, model1, model2) #test difference basemodel versus model 1

#save model 1 (only gesture condition) results 
sum1 <- summary(model1) 
Dmod1 <- lme.dscore(model1, D, type="lme4")

#save model 2 (3way interaction gesture, stress, accent) results 
sum2 <- summary(model2)
Dmod2 <- lme.dscore(model2, D, type="lme4")
posthoc2 <- lsmeans(model2, list(pairwise ~ condition|accent|stress),  adjust="bonferroni")

```
  
  <details><summary>Click here for model 1 R output(model coefficients, effect sizes)</summary>
    ```{r, eval=TRUE}
    sum1
    Dmod1
    ```
  </details>
  
  <details><summary>Click here for more complex model 2 R output (model coefficients, effect sizes, and posthoc)</summary>
    ```{r, eval=TRUE}
    sum2
    Dmod2
    posthoc2
    ```
  </details>
  
## 1B: Does gesture gesturing boost the acoustic markers of stress?

Does gesture vs. no gesture affect acoustic markers of stress? We perform a mixed linear regression with normalized acoustic markers as DV, and acoustic marker (peak F0, peak envelope, and duration) x condition as independent variable. 

Figure 4. Effect of gesture vs. no gesture on acoustic markers of stress 
```{r, echo = FALSE, message= FALSE, warning = FALSE, fig.width = 7}
a <- ggplot(D, aes(x = condition, y = D$peakF0z, color = condition)) + geom_quasirandom(alpha=0.2) + geom_boxplot(alpha = 0, color = "black") + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))+ylab("peak F0 (normalized)")+theme_bw()+theme(legend.position = "none")+ theme(axis.text.x=element_text(angle = -70, hjust = 0))
b <- ggplot(D, aes(x = condition, y = D$peakAMPz, color = condition)) + geom_quasirandom(alpha=0.2) + geom_boxplot(alpha = 0, color = "black") + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))+ylab("peak envelope (normalized)")+theme_bw()+theme(legend.position = "none")+ theme(axis.text.x=element_text(angle = -70, hjust = 0))
c <- ggplot(D, aes(x = condition, y = D$sDURz, color = condition)) + geom_quasirandom(alpha=0.2) + geom_boxplot(alpha = 0, color = "black") + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))+ylab("vocal duration (normalized)")+theme_bw()+theme(legend.position = "none")+ theme(axis.text.x=element_text(angle = -70, hjust = 0))
d <- ggplot(D, aes(x = condition, y = stressSCORE, color = condition)) + geom_quasirandom(alpha=0.2) + geom_boxplot(alpha = 0, color = "black") + scale_color_manual(values=wes_palette(n=2, name = "Royal1"))+ylab("stress score (normalized)")+theme_bw()+theme(legend.position = "none")+ theme(axis.text.x=element_text(angle = -70, hjust = 0))

subplot(ggplotly(a), ggplotly(b), ggplotly(c), ggplotly(d), titleY = TRUE,margin = 0.04)

```

Code chunk 3. Gesture and acoustic output
```{r, eval = TRUE, warning = FALSE}
Dlong <- gather(D, "marker", "acoust_out", 13:15)

#alternative model with gesture versus no gesture as predictor
model0 <- lme(acoust_out~1, data = Dlong, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
model1 <- lme(acoust_out~marker*condition, data = Dlong, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
anova(model0, model1) #test difference basemodel versus model 1

#summary model 3 post hoc
anovamod0mod1 <- anova(model0, model1)
sum1 <- summary(model1)
posthocsum1 <- lsmeans(model1, list(pairwise ~ condition|marker),  adjust="bonferroni")
Dmod1 <- lme.dscore(model1, Dlong, type="nlme")

```
<details><summary>Click here for model 1 R output</summary>
    ```{r, eval=TRUE}
    sum1
    ```
</details>

<details><summary>Click here for model 1 summary for research question 2</summary>

  We again test this model against a base model predicting the overall mean. The model with acoustic markers x condition was a more reliable model than the base model predicting the overall mean of the acoustic output, Chi-sq (5) = `r printnum(anovamod0mod1$L.Ratio[2], digits = 3)`, *p* = `r printnum(anovamod0mod1$'p-value'[2], digits = 3)`. Table 3 provides an overview of the model predictors.
  
```{r table03, echo = FALSE}
tab03 <- cbind.data.frame(sum1$tTable, c(NA, Dmod1$d))
  
  
apa_table(
  tab03
  , caption = "Table 3. Model fitted predictions"
)
```

  We will further perform a post-hoc analysis disentangling these interaction effects, where we assess for which acoustic marker gesture vs. no gesture affected acoustic output. Further, if there is an effect of gesture on stress realization, we will assess in a more complex model whether stress mark presence and L1-L2 stress mismatch interact with the gesture condition effect on stress realization. 

</details>

<details><summary>Click here for posthoc model 1 output</summary>
    ```{r, eval=TRUE}
    posthocsum1
    ```
</details>

</details> 

## Research question 2A: Is gesture-prosody synchrony affected by stress-match/mismatch?

From the previous analyses we should know whether stress timing performance and acoustic stress marking increases or decreases as a function of gesture, as well as the possible role of stress difference, and accentedness in stress timing. A further question is whether the timing between gesture and speech is affected by stress difference and accentedness, which would signal that gesture does not simply always synchronize with speech, but that coordination is destabilized due to difficulties of reaching the L2 targets without orthographic cues or with L1 stress competitor.  
	Using a similar linear mixed modeling approach as the previous analysis we compare a base model with models with stress difference and accentedness (and their possible interactions) as predictors for the absolutized gesture-speech asynchrony. 

Figure 5. Gesture-speech (a)synchrony depending on stress difference and accentedness
```{r, echo = FALSE}
subD <- subset(D, condition == "gesture")
a <- ggplot(subD, aes(x= asynchrony, color = stress)) + geom_density(size= 2)+theme_bw()+ylim(1e-12, NA)+facet_grid(.~accent)
a <- a + scale_color_manual(values=wes_palette(n=2, name = "BottleRocket2")) + ggtitle("asynchrony as a function of stress difference and accent")
ggplotly(a)
```

    ```{r, echo=FALSE, eval=TRUE}
subD$abs_asynchrony <- abs(subD$asynchrony)
#alternative model with gesture versus no gesture as predictor
model0 <- lme(abs_asynchrony~1, data = subD, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
model1 <- lme(abs_asynchrony~stress+accent, data = subD, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
model2 <- lme(abs_asynchrony~stress*accent, data = subD, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)

anovmod01 <- anova(model0, model1) #test difference basemodel versus model 1
anovmod02 <- anova(model0, model2) #test difference basemodel versus model 1

#if interaction effects are reliable we will follow up with post-hocs
sum1 <- summary(model1)
sum2 <- summary(model2)
posthoc2 <- lsmeans(model2, list(pairwise ~ stress|accent),  adjust="bonferroni")
Dmod1 <- lme.dscore(model1, subD, type="nlme")

```


<details><summary>Click here for posthoc model 1 and 2 output</summary>
    ```{r, eval=TRUE}
    sum1
    sum2
    posthoc2
    ```
</details>

<details><summary>Click here for model 2 summary for research question 3</summary>

 For our pilot data, including stress difference and accentedness as predictors in an alternative model was not a more reliable than the base model predicting the overall mean of the absolutized gesture-speech (a)synchrony, Chi-sq (2) = `r printnum(anovmod01$L.Ratio[2], digits = 3)`, *p* = `r printnum(anovmod01$'p-value'[2], digits = 3)`, and adding interactions between stress difference and accentedness also did not further improve  predictions of gesture-speech asynchrony, Chi-sq (3) = `r printnum(anovmod02$L.Ratio[2], digits = 3)`, *p* = `r printnum(anovmod02$'p-value'[2], digits = 3)`. Table 4 provides an overview of the model predictors for the model without interactions.
  
```{r table04, echo = FALSE}
tab04 <- cbind.data.frame(sum1$tTable, c(NA, Dmod1$d))
  
apa_table(
  tab04
  , caption = "Table 4. Model fitted predictions"
)
```

</details>


## 2B. Do correct L2 productions still demonstrate evidence of L1 temporal attraction  

From the previous analysis we will know if gesture-speech synchrony can be affected by trial conditions that may complicate correct stress placement. If indeed gesture-speech (a)synchrony is affected, we can wonder about how gesture and speech temporally diverge when they are more asynchronous. We will assess this by looking at the gesture-speech asynchrony when speech stress peak is correctly placed a) on the L2 target (2B), b) when placed incorrectly on L1 target (2C). Figure 6 provides an example of our pilot data results where we report directional gesture-speech (a)synchrony when acoustic stress is correctly placed on the L2 target. We will assess whether gesture is attracted to be asynchronous with speech in the direction of the L1 stress competitor. We compare this directional (a)synchrony when there is a L1/L2 stress difference, versus when there is no stress difference. If there is an attraction of gesture towards L1, we would predict the positive effect of stress difference on the directional gesture-speech (a)synchrony as compared to no stress difference condition. The modeling we will perform is similar to our previous analysis, and are reported here below.


```{r, echo = FALSE}
#for correct placement
Dsub <- subset(D, (condition != "gesture") & (correct == "L2 correct"))
a <- ggplot(Dsub, aes(x = stress, y= asynchrony_L2L1))+geom_hline(yintercept = 0, color = "red", size = 0.5)+geom_violin(fill=NA) + geom_quasirandom(color = "black", size = 0.7, alpha = 0.5)+geom_boxplot(alpha = 0)+theme_bw() + coord_flip()
a <- a +facet_grid(correct~.)
ggplotly(a)
```

Code Chunk 4. Conditional analysis 3a/3b
```{r}
#basemodel predicting the overall asynchrony
model0 <- lme(asynchrony_L2L1~1, data = subD, random = list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)

#alternative model stress versus no stress difference
model1 <- lme(asynchrony_L2L1~stress, data = subD, random =  list(~1|ppn, ~1|target), method = "ML", na.action = na.exclude)
anova(model0, model1) #test difference basemodel versus model 1
summary(model1) 
```


## Power analysis
  To provide some indication on the amount of data we need to collect to get meaningful results, we perform a power analysis concerning the first confirmatory research question. We will assess the power of a model with three main effects ( stress difference, accentedness, and gesture condition) on directional stress timing at an adjusted alpha of .05/3, and identify how many subjects we need to detect a main effects at a power of 80%. We use R-package 'mixedpower' that is designed to simulate data and power of linear mixed effects models from pilot data (see Kumle et al. 2021 for a tutorial). Table 5 shows the power estimates for the effects for N of 20 to 60 participants. It can be seen that for the effect of gesture and stress we already have enough power to detect an actual effect at N = 20 and higher. Given that accentedness is not as important of a variable as gesture condition and stress differences, we can conclude from this that an absolute minimum of 20 participants would suffice for ensuring a meaningful test of our confirmatory research question 1. Note however, that we will aim to collect 40 participants.  
  We further report in Table 6 the power calculations are given for a more complex model with a three-way interaction, and the lower order interaction effects between stress, accentedness, and gesture condition. It can be seen that three-way interactions are likely to have low power, but that two-way interactions could become more meaningful at N = 60. Therefore we will set our ideal upper bound at N = 60.

Table 5. Power analysis simple model
```{r, cache = TRUE}
#for details on this power analysis see
#https://link.springer.com/article/10.3758/s13428-021-01546-0

D$ppn <- as.numeric(as.factor(D$ppn)) #random variable to extend 

#make a lme4 model instead of lme
model1 <- glmer(accuracy ~  condition  + (1 | ppn)+(1|target),data = D, family = binomial(link="logit"))

#Power analysis
power_model1 <- mixedpower(model = model1, data = D,
                        fixed_effects = c("condition"),
                        simvar = "ppn", steps = c(20,30,40,50, 60),
                        critical_value = 2.14441, n_sim = 500)

#plot power analysis results
tab05 <- power_model1
  
apa_table(
  tab05
  , caption = "Power fixed effects for number of participants"
)
```

Table 6. Three way interaction model
```{r, cache = TRUE}
#alternative model with, stress, accentedness, and gesture versus no gesture as predictors
model2 <- glmer(accuracy ~  condition * accent * stress + (1 | ppn)+(1|target),data = D, family = binomial(link="logit"))


#Power analysis 2
power_model2 <- mixedpower(model = model2, data = D,
                        fixed_effects = c("condition", "stress", "accent"),
                        simvar = "ppn", steps = c(20,30,40,50, 60),
                        critical_value = 2.14441, n_sim = 500)


tab06 <- power_model2
  
apa_table(
  tab06
  , caption = "Power fixed effects for number of participants"
)
```
